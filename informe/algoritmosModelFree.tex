
\subsubsection{Introducción}

Agentes Model Free:
Estos agentes no construen un mapa de ambiente sino que eligen la accion a seguir en base a unos valores que van calculando de distintas formas para cada algoritmo La politica de aprendizaje basica es Qlearning, los de mas hacen modificaciones de eso, para el caso de sarsa se agrega x, para sarsa lambda se agrega y sobre sarsa y para dyna se prende fuego la casa de roberto giordano.
\paragraph{}El modelo del problema consiste en el conocimiento de la función de transición de estados y su función de refuerzo. El aprendizaje por refuerzo se ocupa principalmente de obtener una política optima para tal modelo cuando ese modelo no es totalmente conocido. El agente debe interactuar con el entorno directamente para obtener información, la cual puede ser procesada para producir una política optima.

\paragraph{}En este punto hay dos formas de proceder:

\begin{itemize}

	\item Model-Free: Aprender la estrategia sin aprender el modelo.
	\item Model-Based: Aprender el modelo para luego utilizarlo para derivar la estrategia.

\end{itemize}


\subsubsection{QLearning}
	
\paragraph{}	Q-learning es una técnica que funciona aprendiendo una función en base a acciones-valores que devuelve el refuerzo esperado por tomar la acción dada en un estado dado y siguiendo una política fija. Lo importante es que no requiere aprender el modelo del problema.
	
	
\paragraph{}El modelo del problema consiste en un agente, estados S, y un número de acciones por estado A. Realizando las acciones , el agente puede moverse de estado a estado. Cada estado provee al agente de una recompensa (un número natural en nuestro caso) o castigo (una recompensa negativa). El objetivo del agente es maximizar la recompensa total. Lo cual logra aprendiendo que acción es optima para cada estado.

\paragraph{}El algoritmo, por lo tanto, posee una función que calcula la calidad de un par estado-acción.

$Q: S \times A \rightarrow \Re$

\paragraph{}Antes que el aprendizaje empiece, la función Q retorna un valor fijo, elegido por el diseñador del algoritmo. Luego, cada vez que el agente recibe una recompensa, nuevos valores son calculados para cada combinación de estado-acción de S x A. El núcleo del algoritmo es una simple actualización iterada de valores, la cual actualiza los valores en base a nueva información.


$$Q(s_t,a_t) \leftarrow \underbrace{Q(s_t,a_t)}_{\footnotesize{\textrm{ viejo valor}}} + \underbrace{\alpha}_{\footnotesize{\textrm{tasa aprendizaje}}}*\left[ \overbrace{ \underbrace{r_{t+1}}_{\footnotesize{\textrm{ recompenza}}} + \underbrace{\gamma}_{\footnotesize{\textrm{factor de descuento}}}* \underbrace{\max_aQ(s_{t+1},a)}_{\footnotesize{\textrm{máximo valor futuro}}}}^{\footnotesize{\textrm{recompenza descontada esperada}}} - \underbrace{Q(s_t,a_t)}_{\footnotesize{\textrm{ viejo valor}}}\right]  $$


Un episodio del algoritmo termina cuando el estado $s_{t+1}$ es un estado final.


\paragraph{Influencia de las variables en el algoritmo}
\paragraph{}\textbf{$\alpha$ (Learning rate)}\\
 La tasa de aprendizaje es el que determina que importancia darle a la nueva información, un factor cerca de 0 implica que el agente aprenda muy poco y una tasa cerca de 1 implica que el agente considerará mayormente la información reciente.

\paragraph{}\textbf{$\delta$ (Discount factor)}\\
 El factor de descuento determina la importancia de futuras recompensas. Un factor cerca de cero hará que el agente considere solo su acción en el momento y no sus consecuencias futuras, contrario al caso de un factor cerca de uno donde el agente considerara las recompensas a largo plazo. (si el factor es mayor a uno Qleaning diverge)

\paragraph{Implementación}
\paragraph{}Q learning usa una tabla para almacenar los datos obtenidos, en nuestro caso un simple diccionario con claves (acción,estado) y el valor de la función Q como valor. A continuación, veremos el pseudocódigo utilizado:

	
		\begin{figure}[h!]
			\centering
			\includegraphics[width=0.6\textwidth]{qLearning.png}
			\caption{Algoritmo QLearning}
		\end{figure}
	
	En nuestro caso, la estrategia utilizada para escoger la nueva acción es $\epsilon$-Greedy con $\epsilon$ valiendo 0.1, el cual funciona de la siguiente manera:
	
	
próxima acción a tomar =  maxarg$_a\{Q(s,a)\}$ con probabilidad (1-e) y una acción al azar con probabilidad e.

	

\paragraph{}Por otra parte, las variables que modifican el comportamiento del algoritmo fueron seteadas en:


\begin{itemize}
	\subsubsection{Sarsa (State-Action-Reward-State-Action)}
	
	\item LEARNING\_RATE $= \alpha = 0.8 $
	\item DISCOUNT\_FACTOR $= \delta = 0.95 $
\end{itemize}	
	
\subsubsection{Sarsa (State-Action-Reward-State-Action)}

Sarsa es otro algoritmo libre de modelo la cual también utiliza la función de valores Q.

El nombre, refleja el hecho que la función principal para actualizar los Q-valores depende en el estado actual, la acción que elije, el reward que obtiene, el estado en el que se encuentra luego de realizar la acción y finalmente la acción la cual el agente va a elegir en su próximo estado.

Un agente Sarsa interectuará con el entorno y actualizará la política en base a acciones tomadas anteriormente. Los valores de la función Q son actualizados en base al error de predicción (ajustado por la tasa de aprendizaje ($\alpha$)).
Los valores de Q representan el posible reward a recibir en el próximo paso luego de tomar la acción actual en el estado actual sumado a el valor de recompenza futuro descontado  recibido de la próxima observación de acción-estado. Veamos el pseudocódigo del algoritmo:



		\begin{figure}[h!]
		\centering
		\includegraphics[width=0.6\textwidth]{Sarsa.png}
		\caption{Algoritmo Sarsa}
		\end{figure}
	
	
\paragraph{} Para la implementación, los parametros fueron seteados en:



\begin{itemize}
	
	\item LEARNING\_RATE $= \alpha = 0.8 $
	\item DISCOUNT\_FACTOR $= \delta = 0.95 $
\end{itemize}	
	
	
	
	\subsubsection{Sarsa ($\lambda$)}	
\paragraph{} En los algoritmos vistos, la actualización se hace utilizando únicamente la siguiente recompensa.
\paragraph{}En este algoritmo, se utilizan las trazas de elegibilidad, la idea de ellas es considerar las recompensas de n estados posteriores (o afectar a n anteriores).

\paragraph{}En la práctica, más que esperar n pasos para actualizar (forward view), se realiza al revés (backward view). Se guarda información sobre los estados por los que se pasó y se actualizan hacia atrás las recompensas (descontadas por la distancia). Se puede probar que ambos enfoques son equivalentes.


\paragraph{}Para implementar la idea anterior, se asocia a cada estado o par estado-acción
una variable extra, representando su traza de elegibilidad (eligibility trace)
que denotaremos por $e_t(s)$ o $e_t(s, a).$

\paragraph{}Este valor va decayendo con la longitud de la traza creada en cada episodio. 

\paragraph{}Para SARSA se tiene lo siguiente:

$$ e_t(s, a) = \gamma \lambda  e_t(s, a) \textrm{\hspace{2cm}si } s \neq s_t$$
$$ e_t(s, a) = \gamma \lambda  e_t(s, a)+1 \textrm{ \hspace{2cm}si } s = s_t$$

\paragraph{}Obteniendo el siguiente algoritmo: 



		\begin{figure}[h!]
			\centering
			\includegraphics[width=0.8\textwidth]{SarsaLambda.png}
			\caption{Algoritmo Sarsa ($\lambda$)}

		\end{figure}
	
	
\paragraph{}Aquí se puede mantener historia de la traza solo hasta el primer movimiento exploratorio, ignorar las acciones exploratorias, o hacer un esquema un poco más complicado que considera todas las posibles acciones en cada estado.
	
	
\paragraph{}Aquí se podría mantener historia de la traza solo hasta el primer movimiento exploratorio, ignorar las acciones exploratorias, o hacer un esquema un poco más complicado que considera todas las posibles acciones en cada estado.


\paragraph{} Para nuestra implementación, los parametros fueron seteados en:

\begin{itemize}
	
	\item DECAY\_FACTOR $= \lambda = 0.9 $
	\item LEARNING\_RATE $= \alpha = 0.8 $
	\item DISCOUNT\_FACTOR $= \delta = 0.95 $
\end{itemize}	
	
	
	