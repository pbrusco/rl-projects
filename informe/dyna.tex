
	\subsubsection{Dyna-Q}
		
		
\paragraph{}Asumamos que tenemos un modelo del ambiente, esto es, que podemos predecir el siguiente estado y la recompensa dado un estado y una acción. La predicción puede ser un conjunto de posibles estados con su probabilidad asociada o puede ser un estado que es muestreado de acuerdo a la distribución de probabilidad de los estados resultantes. Dado un modelo, es posible hacer planificación. Lo interesante es que podemos utilizar los estados y acciones utilizados en la planificación también para aprender. De hecho al sistema de aprendizaje no le importa si los pares estado-acción son dados de experiencias reales o simuladas. Dado un modelo del ambiente, uno podría seleccionar aleatoriamente un par (estado,acción), usar el modelo para predecir el siguiente estado, obtener una recompensa y actualizar valores Q. Esto se puede repetir indefinidamente hasta converger a Q\*.

\paragraph{}El algoritmo Dyna-Q combina experiencias con planificación para aprender más rápidamente una política óptima.
La idea es aprender de experiencia, pero también usar un modelo para simular experiencia adicional y así aprender más rápidamente 

\paragraph{}El algoritmo de Dyna-Q selecciona pares estado-acción aleatoriamente de pares anteriores. Sin embargo, la planificación se puede usar mucho mejor si se enfoca a pares estado-acción específicos. Por ejemplo, enfocarnos en las metas e irnos hacia atrás o más generalmente, irnos hacia atrás de cualquier estado que cambie su valor.Los cambios en las estimaciones de valor V o Q pueden cambiar, cuando se está aprendiendo o si el ambiente cambia y un valor estimado deja de ser cierto.

\paragraph{}Veamos el pseudocódigo:
		
		\begin{figure}[h!]
			\centering
			\includegraphics[width=0.6\textwidth]{Dyna.png}
			\caption{Algoritmo Dyna-Q}
		\end{figure}
	
	