\subsubsection{RMax}

\paragraph{}Para implementar Rmax en nuestro trabajo, decidimos basarnos en la versión de KWIK-Rmax presente en las slides de la quinta clase del curso. Además, para más detalle sobre KWIK, y sobre las cotas matemáticas necesarias, nos basamos fuertemente en una disertación de Lihong Li \cite{li}.

\paragraph{}Un agente Rmax, al ser model based, va armando un modelo completo en base a la experiencia adquirida y, al momento de decidir qué acción tomar, elige aquélla que considera más valiosa en base a todo el modelo que tiene computado el agente. Entonces, el algoritmo se divide en dos fases: una fase de aprendizaje, donde actualizamos el modelo en base a la experiencia empírica, y la fase de toma de decisión, donde, en base al modelo, mediante alguna técnica como value iteration, decidimos qué acción es la que más conviene tomar.

\paragraph{}A continuación se presenta el pseudocódigo de KWIK-Rmax extraído de la disertación Li \cite{li}, página 145:

	\clearpage
		\begin{figure}[h!]
			\centering
			\includegraphics[width=0.6\textwidth]{kwik-rmax.png}
			\caption{kwik-rmax}
		\end{figure}

\paragraph{}En el algoritmo, es la línea 12 donde se decide la próxima acción. En la línea 13 obtenemos el reward y el estado siguiente, valores que se usan a continuación para la fase de aprendizaje. \textbf{$A_T$} y \textbf{$A_R$} son los algoritmos que, dado un par estado-acción, determinan, respectivamente, el valor empírico de la probabilidad de moverse al estado deseado y el valor empírico del reward esperado, o, si no tienen suficientes datos, dicen que dicho par estado-acción es desconocido. Con una cota suficientemente grande, se garantiza que dicho algoritmo sea PAC dados $\epsilon$ y $\delta$. En la sección 7.2.1, página 148, Li da un valor para esta cota (donde $n$ es la cantidad de estados y $m$ la cantidad de acciones):

\[
	O(\frac{n^2m}{\epsilon^2}ln(\frac{nm}{\delta}))
\]

\paragraph{}Sin embargo, en nuestro caso, tenemos el problema de la gran cantidad de estados posibles: En el tablero de 5 x 5 (el tablero utilizado para todas las pruebas) tenemos: 21 casilleros posibles para el agente, 21 casilleros posibles para la bomba, $2^5$ estados de paredes explotadas. Lo que da un total de 21 x 21 x 32 = 14112 estados. Entonces, asumiendo $\epsilon = 1$ y $\gamma = 1$, la cuenta de la cota igualmente es la siguiente (recordar que tenemos 6 acciones posibles):
\[
	14000^2\cdot6\cdot ln(14000\cdot6) \geq 10^9
\]

Este número resulta claramente inmanejable, no se puede pretender pasar por un par estado-acción tantas veces para que sea considerado conocido. Entonces, tuvimos que pensar otras alternativas para nuestro trabajo. Tendremos que contentarnos con un Rmax que no sea PAC.

\paragraph{}Una primera alternativa fue simplemente bajar la cota para que un estado pase a ser conocido, tomando otros valores de referencia (la dimensión del tablero nada más por ejemplo). Esto nos trae el problema de definir arbitrariamente este valor. Un valor muy grande puede hacer que tarde mucho en aprender el modelo (pues necesita muchas corridas para considerar que un par estado-acción es conocido), y un valor muy chico puede contentarse con valores más alejados de la realidad. Al final, implementamos Rmax de la manera siguiente:

\paragraph{}Un par estado-acción es considerado conocido con que se pase por él por lo menos una vez. Sin embargo, siempre que se pasa por dicho par se actualiza su valor empírico (a diferencia de KWIK donde, una vez algo es conocido, no se actualiza más). Esto hace que tengamos que correr Value iteration cada vez pero, si casi no cambiaron los valores, en una iteración se resuelve Value iteration.

\paragraph{}\textbf{Detalles de implementación:}

\paragraph{}

Para decidir qué acción tomar, utilizamos el algoritmo de value iteration presentado en la segunda clase del curso. Sin embargo, en vez de iterar por todos los posibles estados en el doble ciclo, llevamos almacenados dos conjuntos: una para estados desde los cuales realizamos por lo menos una vez una acción (por lo que tenemos cierta información), a este conjunto lo llamaremos \textbf{visitados}; y otro de los estados que sabemos su existencia (o sea, \textbf{visitados} más lo que alguna vez llegamos pero no realizamos ninguna acción desde ellos), a este conjunto lo llamaremos \textbf{alcanzables}. Esto lo hicimos pues era una locura iterar por todos los posibles estados en dos ciclos anidados y, además, nos parece coherente que el agente sólo calcule a partir de los estados que sabe que existen y no con el conjunto total. Los estados en los cuales vamos a iterar su valor calculado son los pertenecientes a \textbf{visitados}, pues vamos a asumir Vmax para los demás (para los que pertenecen a \textbf{alcanzables} y no a \textbf{visitados}), pues, al no tener información alguna sobre ellos (más allá de su existencia), siempre suponemos Vmax, por lo que no tiene sentido incluirlos en las iteraciones de Value iteration. Entonces, las guardas de los ciclos quedarían así (código python, extraído de RmaxAgent.py):

\begin{verbatim}

for visited in self.visitedStates:
  for action in Action.ACTIONS:
    #inicializo con el reward dado el estado y la accion
    partialQ = self.getRValue(action, visited)
    for possible in self.reachableStates: #todos los estados que se que existen

\end{verbatim}

Por otro lado, se almacena en un diccionario los valores de los estados pertenecientes a \textbf{visitados}. O sea, dado un estado como clave, se devuelve su valor. De esta manera, los valores iniciales para Value Iteration se setean a partir de este diccionario, pues, para cada acción, lo normal es que el valor de todos los estados casi no cambie. De este modo, se reduce fuertemente la cantidad de veces que hay que iterar en Value Iteration.

\subsubsection{RMax Factorizado}

\input{dyna.tex}