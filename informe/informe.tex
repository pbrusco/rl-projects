\documentclass[a4paper,spanish] {article} 
\usepackage [spanish] {babel} 
\usepackage [latin1]{inputenc}
\usepackage{graphicx}
\usepackage{caratula}
\usepackage{subfig}
\usepackage{dsfont}
\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{algorithmic}

\addtolength{\oddsidemargin}{-1in}
\addtolength{\textwidth}{2in}

\begin{document}
\pagestyle{headings}



\newpage

\materia{Aprendizaje por Refuerzos: Teor\'ia y Aplicaciones en Rob\'otica, Psicolog\'ia y Neurociencias}
\submateria{Tp Final}
\titulo{Desarrollo de algoritmos de aprendizaje y an\'alisis de resultados en una adaptaci\'on del problema Bomberman}

\integrante{Pablo Brusco}{527/08}{pablo.brusco@gmail.com}
\integrante{Carolina Hadad}{367/08}{carolinahadad@gmail.com}
\integrante{Sergio Medina}{216/05}{medinasergio@gmail.com}
\integrante{Santiago Palladino}{138/05}{spalladino@gmail.com}
\integrante{Andres Taraciuk}{333}{ataraciuk@gmail.com}


\maketitle

		
\newpage
\tableofcontents
\newpage

\section{Objetivo}
	En  este Trabajo Practico nos interesa analizar el desempe\~ no de los algoritmos de aprendizaje vistos durante el curso,  aplic\'andolos  en un problema distinto de los planteados en las clases. En nuestro caso, elegimos adaptar el juego del Bomberman. 
	
	Compararemos los resultados del aprendizaje en agentes tanto model-free como model-based, con los algoritmos de QLearning, RMax, Sarsa, Sarsa Lambda, y Dyna mostrando cu\'ales de ellos funcionan mejor en nuestro caso. Intentaremos explicar tambi\'en a qu\'e se deben estos resultados. Adem\'as analizaremos el efecto de aplicar rewards intermedios en el tiempo de aprendizaje del agente y c\'omo var\'ia el aprendizaje del agente al moverse en un ambiente estoc\'astico con distintas condiciones de aleatoriedad.

\section{Introducci\'on}
	\subsection{Adaptaci\'on del juego del Bomberman}
	El juego que vamos usar es una versi\'on simplificada del juego del Bomberman. El objetivo del agente es llegar a la salida. El agente tiene 4 acciones de movimiento: arriba, abajo, hacia la izquierda y hacia la derecha. El tablero tiene paredes que obstaculizan su camino, algunas de ellas son rompibles y otras irrompibles. El agente tiene acciones para poner una bomba y para explotarla. Solo puede haber una bomba en el tablero, por lo que si el agente realiza una acci\'on de tirar bomba habiendo una bomba en el juego, la acci\'on no tendr\'a efecto. Al explotar la bomba se destruir\'an las paredes que est\'en arriba, abajo a la izquierda y a la derecha de ella. Si el agente estuviera en alguno de estos lugares o sobre la bomba, el agente morir\'ia, teniendo que empezar el juego nuevamente desde la posici\'on inicial. 
	
	Nos interesa que el agente aprenda una pol\'itica para llegar a la salida realizando la menor cantidad de acciones posibles, para esto le damos un refuerzo positivo grande en el momento que llega al casillero de salida y un refuerzo negativo de la misma magnitud cuando el agente muere. A estos refuerzos los llamaremos WIN\_REWARD y LOSE\_REWARD respectivamente.
	
\section{Representación del estado}
	En nuestro programa vamos a representar el Estado como una tupla con los valores que listaremos ahora:
\begin{itemize}
\item \textbf{bombermanPos}: tupla con la posición del agente, en x y en y en un momento determinado. Valor inicial (0,0)
\item \textbf{bomb}: tupla que indica la posicion en x y en y de la bomba en el tablero, si es que la hay. None si no hay bomba.
\item \textbf{isBombDropped}: Valor booleano que indica si el agente lanzó una bomba pero todavía no la detonó.
\item \textbf{stones}: arreglo de valores booleanos. Cada posición i del arreglo indica si esa pared rompible está presente en el tablero, es decir que aún no fue destruida por el agente. El orden de las paredes va de arriba ala izquierda a abajo a la izquierda. Inicialmente están todas presentes. En el archivo $sample_board.txt$ se puede ver la configuración de los distintos tableros usados.
\item \textbf{die}: Valor booleano que indica si el agente murió.
\item \textbf{deltaBomb}: valor que está en el estado sólo para Rmax. Indica la distancia del agente a la bomba lanzada, si la hubiera.
\end{itemize}

	Serializamos el estado en un número único para .. ?. La implementación de estos métodos se puede ver en el archivo $State.py$, pero consiste principalmente en .. ?.

\section{Acciones posibles del agente}
	El agente cuenta con 4 acciones de movimiento, que se pueden ver en el archivo $Action.py$: UP, DOWN, LEFT y RIGHT. Además puede lanzar una bomba (sólo una por vez, no puede haber 2 bombas activas en el juego) con la acción DROP\_BOMB. La bomba se ubica en la posición en la que estaba el agente cuando la lanzó. El agente puede, por último, explotar una bomba lanzada mediante la acción EXPLODE. El radio de alcance de la bomba es de el casillero donde esta ubicada la bomba y sus aledaños, es decir el de arriba, el de abajo, el de la izquierda y el de la derecha. Cuando una bomba explota destruye las paredes rompibles (STONES) que estén en su radio de alcance y mata al agente si este se encuentra en este radio. Una vez que la bomba es explotada, el agente puede lanzar otra.



\section{Uso}
	Para correr el programa se debe elegir una configuración para los par\'ametos en alg\'un archivo de la carpeta settings. Luego, agregar el nombre del archivo modificado en la lista de RUN\_CONFIGURATIONS del archivo Runner.py. Luego, basta ejecutar el archivo Runner.py($ejecutar\ python\ Runner.py$)y se correr\'an todas las configuraciones agregadas a la lista de RUN\_CONFIGURATIONS.
	
	Los archivos de settings deben tener valores para los siguientes par\'ametros:
\begin{enumerate}
\item CONFIGURATION\_NAME 
\item ITERATIONS: cantidad de veces que correrá el algoritmo. El algoritmo corre hasta que el agente muere, llega a la salida o llega a la m\'axima cantidad de acciones posible.
\item MAX\_TURNS: M\'axima cantidad de acciones que puede realizar el agente hasta que el juego se de por terminado.
\item AGENT: algoritmo de aprendizaje del agente que correr\'a. Se pueden elegir los siguientes algoritmos:
\begin{itemize}
\item QLEARNING
\item RMAX
\item FACTOREDRMAX 
\item SARSA
\item SARSALAMBDA
\item DYNA
\end{itemize}
	
\item TASK: determina el tipo de ambiente en el que correr\'a el agente. El agente puede correr en un ambiente determin\'istico o estoc\'astico. En los ambientes estoc\'asticos, con cierta probabilidad, las acciones no tienen el efecto esperado. Desarrollamos las siguientes opciones para este par\'ametro:
\begin{itemize}
\item DETERMINISTIC: cada acción causa el mismo resultado siempre.
\item STOCHASTIC\_NAVIGATION: con cierta probabilidad, las acciones de movimiento causan que el agente se mueva en otra direcci\'on. Por ejemplo podria pasar que algente realice la acci\'on UP y que el efecto sea el mismo que el que hubiera causado realizar la acci\'on LEFT en un ambiente deterministico, por ejmplo.
\item STOCHASTIC\_EXPLOSION: Con cierta probabilidad la acci\'on de explotar bomba no la detona. La bomba queda en el mismo lugar en donde el agente la puso. Esto simula el caso en el que el detonador a veces no funciona.
\end{itemize} 
	De todas maneras explicaremos más sobre esto en la sección $Ambiente \ Estocastico$ 

\item IS\_IMMORTAL: si este booleano esta en True, el agente no muere al detonarse una bomba, pese a estar ubicado en su rango de explosión.
\item MAP\_SIZE: El tamaño del mapa. Se puede elegir entre mapas de tamaño 3, 5 u 8. Las distribuciones de walls y stones (paredes irrompibles y rompibles) para cada uno de estos mapas, se puede ver en el archivo sample\_board.txt.
\item BOMB\_EXPLODING\_PROBABILITY: La posibilidad de que las bombas exploten.
\item BOMB\_REWARD\_POLICY: Política de refuerzo intermedio por explotaci\'on de bomba. M\'as adelante en el informe explicaremos cuales son las opciones posibles y qu\'e significa cada una.
\item NAVIGATION\_REWARD: Política de refuerzo intermedio por posici\'on del agente. Tambi\'en explicaremos mas adelante cu\'ales son las opciones para este par\'ametro.
\item WIN\_REWARD: Refuerzo por llegar a la salida.
\item LOSE\_REWARD:Refuerzo negativo por morir.
\item BOMB\_REWARD:Refuerzo intermedio por explotar bomba que se usa de acuerdo a la pol\'itica de refuerzos intermedios adoptada.
\item NO\_ACTION\_NEGATIVE\_REWARD: Si este flag est\'a activado se le da un refuerzo negativo al agente cada vez que realiza una acci\'on que no cambia el estado
\item INITIAL\_REWARD: Refuerzo por acción realizada. Puede estar en 0 o tener un valor negativo.
\item USE\_DELTABOMB\_FACTOR: Booleano que si est\'a en True indica que se debe agregar al estado una tupla con la posición del bomberman respecto de la bomba lanzada. Se usa para Rmax y ser\'a explicado m\'as adelante.
\end{enumerate}
	

\section{Visualizaci\'on}


\section{Arquitectura}
	%pagina http://yuml.me/diagram/scruffy/class/draw
	%codigo [Manager| run()]->1[Task|start(); perform(action)]
	%		[Task] ->1[Environment|start(); performAction(action)]
	%		[Manager]->1[Agent | learn(); nextAction()]
	
	Las clases mas importantes de la arquitectura son: Agent, Task, Environment y Manager.
	
	Como lo indica el modelo conceptual que se muestra debajo, el Manager es la clase que hace interactuar a la Task (y por medio de esta, al Environment) con el Agent. La responsabilidad del Manager es bastante simple: corre un ciclo en el cual ejecuta acciones en la Task, le da la informaci\'on requerida al Agent para que aprenda y luego le indique la próxima acci\'on a tomar.
	El Environment es el encargado de ejecutar acciones en el juego del Bomberman y de mantener su estado, el mismo es agnostico de refuerzos, de los cuales se encarga la Task (la cual es tambi\'en la encargada de implementar aleatoriedad, que se explicar\'a luego).
	
	
	\begin{figure}[h!]
  \centering
    \includegraphics[width=0.5\textwidth]{MCarquitectra.png}
  \caption{Modelo Conceptual de la Arquitectura.}

	\end{figure}
\section{Algoritmos de Aprendizaje}
	Programamos agentes con pol\'iticas de aprendizaje distintas. En esta sección explicaremos en detalle qué agentes hicimos, cómo implementamos cada una de las políticas de aprendizaje y algunas características teóricas de los algoritmos implementados. 
	\subsection{Agentes Model-Free}
		\newpage
		\input{algoritmosModelFree.tex}
	\subsection{Agentes Model-Based}	
		\subsubsection{R-max}
		\subsubsection{R-max factorizado}


\section{Ambiente Estocastico}

	Se implementaron dos versiones que agregan aleatoriedad al juego, en relaci\'on a la navegaci\'on y a la explosi\'on de las bombas.\\
	
	\textbf{Navegaci\'on}: Cuando el Bomberman intenta realizar una acci\'on de navegaci\'on, puede que la misma se ejecute (con una cierta probabilidad) o alguna de otras dos opciones. Estos par\'ametros son configurables pero la configuraci\'on por defecto es la siguiente:
	
\begin{center}
\begin{tabular}{ l | c | r }
  Acci\'on Ejecutada & Opci\'on 1 & Opci\'on 2 \\
  \hline
  Arriba (80\%) & Izquierda (10\%) & Derecha (10\%) \\
  Abajo (80\%) & Izquierda (10\%) & Derecha (10\%) \\
  Izquierda (80\%) & Arriba (10\%) & Abajo (10\%) \\
  Derecha (80\%) & Arriba (10\%) & Abajo (10\%) \\  
\end{tabular}
\end{center}

	La clase encargada de implementar esto es StochasticNavigationTask y se configura seteando $TASK = STOCHASTIC\_NAVIGATION$.\\
	
		\textbf{Explosi\'on}: Cuando el Bomberman intenta explotar una bomba, esta puede explotar o no con cierta probabilidad. Si bien los par\'ametros son configurables la probabilidad de explosi\'on por defecto es 80\%. Se configura seteando $TASK = STOCHASTIC\_EXPLOSION$.
	
 \section{Pol\'iticas de Refuerzo}
 	Para evaluar el efecto de aplicar pol\'iticas de refuerzos intermedios en el tiempo de aprendizaje de los agentes, incluimos los siguientes campos en el archivo de settings, que se agregan al refuerzo por llegar a la salida y al refuerzo negativo en el caso de que el agente muera. En el caso de que el agente llegue a la salida o muera recibe el WIN\_REWARD o el LOSE\_REWARD respectivamente, y no se agrega este refuerzo adicional.

\begin{description}
\item[NAVIGATION\_REWARD] Rewards por posici\'on del agente.
	\begin{itemize}
	\item NAVIGATION\_NO\_REWARD: No se agrega refuerzos intermedios adicionales.
	\item NAVIGATION\_REWARD\_PROPORTIONAL\_TO\_EXIT: Siempre que el agente realice una acci\'on que cambie su posici\'on recibe un refuerzo adicional proporcional a su distancia a la salida. En el caso de las acciones de movimiento que no cambien el estado, como por ejemplo intentar moverse hacia una posici\'on que tenga una pared, el agente no recibir\'a ning\'un refuerzo, de manera de desalentar este tipo de acciones. 
	
	Las formula para el c\'alculo de este refuerzo intermedio es la que sigue:
	$$ refuerzo = cercania\_en\_x + cercania\_en\_y $$
	donde
	$$ cercania\_en\_x =\mid MAP\_SIZE-1 -(EXIT_0 - agentPosition_0) \mid$$
	$$ cercania\_en\_y =\mid MAP\_SIZE-1 -(EXIT_1 - agentPosition_1) \mid$$
	siendo $EXIT = (EXIT_0, EXIT_1)$ la posici\'on de la salida en el tablero
	
	y $agentPosition= (agentPosition_0, agentPosition_1)$
 la posici\'on del agente.
	
	Por ejemplo para un tablero de $8 \times 8$ con la salida en $(7,7)$ los refuerzos que recibir\'ia el agente al llegar a cada posici\'on ser\'ian estos:
	\begin{figure}[h!]
  \centering
    \includegraphics[scale=0.25]{refuerzos.jpg}
  \caption{Refuerzos para un tablero de 8 por 8 con salida en (7,7).}
	\end{figure}	
	La motivaci\'on de este refuerzo es que el agente trate de dirigirse hacia la salida.
	\end{itemize}

\item[BOMB\_REWARD\_POLICY] Rewards por destruir paredes.
	\begin{itemize}
	\item BOMB\_NO\_REWARD: No se agrega refuerzos intermedios adicionales.
	\item BOMB\_REWARD\_PER\_STONE\_DESTROYED: Cada vez que el agente destruye una pared, sin morir, recibe un refuerzo igual al BOMB\_REWARD del archivo de settings. La idea de este refuerzo es que el agente aprenda que debe destruir paredes.
	\item BOMB\_REWARD\_PER\_STONE\_DESTROYED\_PROPORTIONAL\_TO\_EXIT: el refuerzo anterior, usado solo podr\'ia hacer que el agente, por lo menos en las primeras iteraciones, intente romper muchas paredes. Por eso agregamos esta opci\'on, que le da mayor valor a romper paredes cercanas a la salida. La formula para este valor es $BOMB\_REWARD \times cercania\_a\_la\_salida$ calculando la cercan\'ia de la misma forma que antes.
	\end{itemize}

	
\item[NO\_ACTION\_NEGATIVE\_REWARD] Para desalentar el que el agente realizara acciones que no modificaran el estado, agregamos este booleano. Si se activa, cada vez que el agente realiza una acci\'on sin efecto, se le da un refuerzo negativo. Por defecto, cada vez que el agente realice una acci\'on que no cambie el estado, se le dar\'a un refuerzo negativo de -10.

\item[INITIAL\_REWARD] En la misma l\'inea que el \'item anterior, tambi\'en permitimos variar el refuerzo que se le da al agente al realizar cualquier acci\'on. La idea es que si este valor fuera negativo, el agente intentar\'a llegar lo mas r\'apido a la salida. Esta idea se integra al $\gamma$, pero permite explicitar el valor negativo de realizar acciones. Por defecto, el agente recibe un -1 por cada acci\'on realizada.

\end{description}

	Las funciones para la aplicaci\'on de estos refuerzos se pueden encontrar en el archivo $Rewards.py$ y son llamadas desde Task de acuerdo a las pol\'iticas elegidas por el usuario en el archivo de Settings.
 
\section{Pruebas y resultados}
	\subsection{Sin Rewards Intermedios}
	\subsection{Rewards Intermedios por posici\'on del bomberman}
	\subsection{Rewards Intermedios por explotar bomba}
	\subsection{Rewards Intermedios por explotar bomba relativo a la posicion de la bomba}

\section{Conclusiones}
	
\end{document}